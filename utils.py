"""
utils - æ•°æ®åˆ†ææ™ºèƒ½ä½“ä½¿ç”¨çš„å·¥å…·å‡½æ•°
"""
import json
import pandas as pd
import openpyxl
from typing import List, Dict, Union, Optional
import re
import os
from datetime import datetime
import sqlite3
from pathlib import Path

from langchain_openai import ChatOpenAI
from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent

PROMPT_TEMPLATE = """ä½ æ˜¯ä¸€ä½æ•°æ®åˆ†æåŠ©æ‰‹ï¼Œä½ çš„å›åº”å†…å®¹å–å†³äºç”¨æˆ·çš„è¯·æ±‚å†…å®¹ï¼Œè¯·æŒ‰ç…§ä¸‹é¢çš„æ­¥éª¤å¤„ç†ç”¨æˆ·è¯·æ±‚ï¼š
1. æ€è€ƒé˜¶æ®µ (Thought) ï¼šå…ˆåˆ†æç”¨æˆ·è¯·æ±‚ç±»å‹ï¼ˆæ–‡å­—å›ç­”/è¡¨æ ¼/å›¾è¡¨ï¼‰ï¼Œå¹¶éªŒè¯æ•°æ®ç±»å‹æ˜¯å¦åŒ¹é…ã€‚
2. è¡ŒåŠ¨é˜¶æ®µ (Action) ï¼šæ ¹æ®åˆ†æç»“æœé€‰æ‹©ä»¥ä¸‹ä¸¥æ ¼å¯¹åº”çš„æ ¼å¼ã€‚
   - çº¯æ–‡å­—å›ç­”:
     {"answer": "ä¸è¶…è¿‡50ä¸ªå­—ç¬¦çš„æ˜ç¡®ç­”æ¡ˆ"}

   - è¡¨æ ¼æ•°æ®ï¼š
     {"table":{"columns":["åˆ—å1", "åˆ—å2", ...], "data":[["ç¬¬ä¸€è¡Œå€¼1", "å€¼2", ...], ["ç¬¬äºŒè¡Œå€¼1", "å€¼2", ...]]}}

   - æŸ±çŠ¶å›¾
     {"bar":{"columns": ["A", "B", "C", ...], "data":[35, 42, 29, ...]}}

   - æŠ˜çº¿å›¾
     {"line":{"columns": ["A", "B", "C", ...], "data": [35, 42, 29, ...]}}
     
3. æ ¼å¼æ ¡éªŒè¦æ±‚
   - å­—ç¬¦ä¸²å€¼å¿…é¡»ä½¿ç”¨è‹±æ–‡åŒå¼•å·
   - æ•°å€¼ç±»å‹ä¸å¾—æ·»åŠ å¼•å·
   - ç¡®ä¿æ•°ç»„é—­åˆæ— é—æ¼
   é”™è¯¯æ¡ˆä¾‹ï¼š{'columns':['Product', 'Sales'], data:[[A001, 200]]}
   æ­£ç¡®æ¡ˆä¾‹ï¼š{"columns":["product", "sales"], "data":[["A001", 200]]}

æ³¨æ„ï¼šå“åº”æ•°æ®çš„"output"ä¸­ä¸è¦æœ‰æ¢è¡Œç¬¦ã€åˆ¶è¡¨ç¬¦ä»¥åŠå…¶ä»–æ ¼å¼ç¬¦å·ã€‚

å½“å‰ç”¨æˆ·è¯·æ±‚å¦‚ä¸‹ï¼š\n"""


def dataframe_agent(df, query, model_config=None, api_key=None):
    """æ•°æ®åˆ†ææ™ºèƒ½ä½“
    
    Args:
        df: pandas DataFrame
        query: ç”¨æˆ·æŸ¥è¯¢
        model_config: æ¨¡å‹é…ç½®å­—å…¸ï¼ŒåŒ…å«provider, model, base_urlç­‰ä¿¡æ¯
        api_key: ç”¨æˆ·è¾“å…¥çš„APIå¯†é’¥
    """
    # ä¸å†ä»ç¯å¢ƒå˜é‡åŠ è½½ï¼Œä½¿ç”¨ç”¨æˆ·æä¾›çš„APIå¯†é’¥
    if not api_key:
        return {"answer": "è¯·æä¾›æœ‰æ•ˆçš„APIå¯†é’¥ï¼"}
    
    # é»˜è®¤ä½¿ç”¨DeepSeekæ¨¡å‹
    if model_config is None:
        model_config = {
            "provider": "deepseek", 
            "model": "deepseek-reasoner", 
            "base_url": "https://api.deepseek.com/"
        }
    
    try:
        # æ ¹æ®ä¸åŒçš„æä¾›å•†åˆ›å»ºæ¨¡å‹å®ä¾‹
        if model_config["provider"] in ["deepseek", "openai"]:
            model = ChatOpenAI(
                base_url=model_config["base_url"],
                model=model_config["model"],
                api_key=api_key,
                temperature=0,
                max_tokens=8192
            )
        elif model_config["provider"] == "anthropic":
            # Claudeæ¨¡å‹éœ€è¦ä¸åŒçš„é…ç½®
            from langchain_anthropic import ChatAnthropic
            model = ChatAnthropic(
                model=model_config["model"],
                api_key=api_key,
                temperature=0,
                max_tokens=8192
            )
        else:
            # å¯¹äºå…¶ä»–æä¾›å•†ï¼Œæš‚æ—¶ä½¿ç”¨OpenAIå…¼å®¹æ¥å£
            model = ChatOpenAI(
                base_url=model_config["base_url"],
                model=model_config["model"],
                api_key=api_key,
                temperature=0,
                max_tokens=8192
            )
        
        agent = create_pandas_dataframe_agent(
            llm=model,
            df=df,
            agent_executor_kwargs={"handle_parsing_errors": True},
            max_iterations=32,
            allow_dangerous_code=True,
            verbose=True
        )

        prompt = PROMPT_TEMPLATE + query
        response = agent.invoke({"input": prompt})
        return json.loads(response["output"])
        
    except ImportError as e:
        print(f"æ¨¡å‹å¯¼å…¥é”™è¯¯: {e}")
        return {"answer": f"å½“å‰æ¨¡å‹ {model_config['model']} æš‚ä¸æ”¯æŒï¼Œè¯·é€‰æ‹©å…¶ä»–æ¨¡å‹æˆ–å®‰è£…ç›¸åº”ä¾èµ–åŒ…ï¼"}
    except Exception as err:
        error_msg = str(err).lower()
        print(f"åˆ†æé”™è¯¯: {err}")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯APIå¯†é’¥ç›¸å…³é”™è¯¯
        if any(keyword in error_msg for keyword in ['api key', 'apikey', 'api_key', 'unauthorized', '401', 'authentication', 'invalid key', 'incorrect api key']):
            return {"answer": "âŒ APIå¯†é’¥æ— æ•ˆæˆ–ä¸æ­£ç¡®ï¼Œè¯·æ£€æŸ¥å¹¶é‡æ–°è¾“å…¥æ­£ç¡®çš„APIå¯†é’¥ï¼"}
        elif any(keyword in error_msg for keyword in ['quota', 'limit', 'billing', 'insufficient']):
            return {"answer": "âš ï¸ APIé…é¢ä¸è¶³æˆ–è´¦æˆ·ä½™é¢ä¸å¤Ÿï¼Œè¯·æ£€æŸ¥æ‚¨çš„è´¦æˆ·çŠ¶æ€ï¼"}
        elif any(keyword in error_msg for keyword in ['network', 'connection', 'timeout', 'unreachable']):
            return {"answer": "ğŸŒ ç½‘ç»œè¿æ¥é”™è¯¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥åé‡è¯•ï¼"}
        else:
             return {"answer": "æš‚æ—¶æ— æ³•æä¾›åˆ†æç»“æœï¼Œè¯·ç¨åé‡è¯•æˆ–å°è¯•å…¶ä»–æ¨¡å‹ï¼"}


def test_api_connection(model_config, api_key):
    """æµ‹è¯•APIå¯†é’¥è¿æ¥
    
    Args:
        model_config: æ¨¡å‹é…ç½®å­—å…¸
        api_key: APIå¯†é’¥
    
    Returns:
        dict: {"success": bool, "error": str}
    """
    try:
        # æ ¹æ®ä¸åŒçš„æä¾›å•†åˆ›å»ºæ¨¡å‹å®ä¾‹è¿›è¡Œæµ‹è¯•
        if model_config["provider"] in ["deepseek", "openai"]:
            model = ChatOpenAI(
                base_url=model_config["base_url"],
                model=model_config["model"],
                api_key=api_key,
                temperature=0,
                max_tokens=10
            )
        elif model_config["provider"] == "anthropic":
            from langchain_anthropic import ChatAnthropic
            model = ChatAnthropic(
                model=model_config["model"],
                api_key=api_key,
                temperature=0,
                max_tokens=10
            )
        else:
            model = ChatOpenAI(
                base_url=model_config["base_url"],
                model=model_config["model"],
                api_key=api_key,
                temperature=0,
                max_tokens=10
            )
        
        # å‘é€ä¸€ä¸ªç®€å•çš„æµ‹è¯•è¯·æ±‚
        response = model.invoke("Hello")
        return {"success": True, "error": ""}
        
    except Exception as e:
        error_msg = str(e).lower()
        if any(keyword in error_msg for keyword in ['api key', 'apikey', 'api_key', 'unauthorized', '401', 'authentication', 'invalid key', 'incorrect api key']):
            return {"success": False, "error": "APIå¯†é’¥æ— æ•ˆæˆ–ä¸æ­£ç¡®"}
        elif any(keyword in error_msg for keyword in ['quota', 'limit', 'billing', 'insufficient']):
            return {"success": False, "error": "APIé…é¢ä¸è¶³æˆ–è´¦æˆ·ä½™é¢ä¸å¤Ÿ"}
        elif any(keyword in error_msg for keyword in ['network', 'connection', 'timeout', 'unreachable']):
            return {"success": False, "error": "ç½‘ç»œè¿æ¥é”™è¯¯"}
        else:
            return {"success": False, "error": f"è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}"}


def merge_multiple_files(file_list: List[Dict], merge_type: str = "concat") -> pd.DataFrame:
    """åˆå¹¶å¤šä¸ªæ•°æ®æ–‡ä»¶
    
    Args:
        file_list: æ–‡ä»¶åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« {'file': file_object, 'type': 'excel/csv', 'sheet': sheet_name}
        merge_type: åˆå¹¶æ–¹å¼ ('concat': çºµå‘åˆå¹¶, 'join': æ¨ªå‘è¿æ¥)
    
    Returns:
        pd.DataFrame: åˆå¹¶åçš„æ•°æ®æ¡†
    """
    dataframes = []
    
    for file_info in file_list:
        try:
            file_obj = file_info['file']
            file_type = file_info['type']
            
            if file_type == 'excel':
                sheet_name = file_info.get('sheet', 0)
                # é‡ç½®æ–‡ä»¶æŒ‡é’ˆåˆ°å¼€å§‹ä½ç½®
                if hasattr(file_obj, 'seek'):
                    file_obj.seek(0)
                
                # å°è¯•å¤šç§å¼•æ“è¯»å–Excelæ–‡ä»¶
                df = None
                engines = ['openpyxl', 'xlrd', None]
                
                for engine in engines:
                    try:
                        if hasattr(file_obj, 'seek'):
                            file_obj.seek(0)
                        if engine:
                            df = pd.read_excel(file_obj, sheet_name=sheet_name, engine=engine)
                        else:
                            df = pd.read_excel(file_obj, sheet_name=sheet_name)
                        break
                    except Exception:
                        continue
                
                if df is None:
                    raise Exception(f"æ— æ³•è¯»å–Excelæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼")
            elif file_type == 'csv':
                # é‡ç½®æ–‡ä»¶æŒ‡é’ˆåˆ°å¼€å§‹ä½ç½®
                if hasattr(file_obj, 'seek'):
                    file_obj.seek(0)
                df = pd.read_csv(file_obj)
            else:
                continue
                
            # æ·»åŠ æ–‡ä»¶æ¥æºæ ‡è¯†
            df['æ•°æ®æ¥æº'] = file_obj.name if hasattr(file_obj, 'name') else f"æ–‡ä»¶{len(dataframes)+1}"
            dataframes.append(df)
            
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
            continue
    
    if not dataframes:
        return pd.DataFrame()
    
    if merge_type == "concat":
        # çºµå‘åˆå¹¶ï¼ˆè¿½åŠ è¡Œï¼‰
        return pd.concat(dataframes, ignore_index=True, sort=False)
    elif merge_type == "join":
        # æ¨ªå‘è¿æ¥ï¼ˆåŸºäºç´¢å¼•ï¼‰
        result = dataframes[0]
        for df in dataframes[1:]:
            result = result.join(df, rsuffix='_merged')
        return result
    else:
        return dataframes[0]


def join_dataframes(df1: pd.DataFrame, df2: pd.DataFrame, 
                   join_column: str, join_type: str = "inner") -> pd.DataFrame:
    """æ•°æ®è¡¨è¿æ¥åŠŸèƒ½
    
    Args:
        df1: å·¦è¡¨
        df2: å³è¡¨
        join_column: è¿æ¥å­—æ®µ
        join_type: è¿æ¥ç±»å‹ ('inner', 'left', 'right', 'outer')
    
    Returns:
        pd.DataFrame: è¿æ¥åçš„æ•°æ®æ¡†
    """
    try:
        if join_column not in df1.columns:
            raise ValueError(f"å·¦è¡¨ä¸­ä¸å­˜åœ¨å­—æ®µ: {join_column}")
        if join_column not in df2.columns:
            raise ValueError(f"å³è¡¨ä¸­ä¸å­˜åœ¨å­—æ®µ: {join_column}")
            
        return pd.merge(df1, df2, on=join_column, how=join_type, suffixes=('_å·¦è¡¨', '_å³è¡¨'))
    except Exception as e:
        print(f"æ•°æ®è¡¨è¿æ¥å¤±è´¥: {e}")
        return df1


def analyze_mixed_format_data(files_data: List[Dict], analysis_query: str, 
                             model_config: Dict = None, api_key: str = None) -> Dict:
    """æ”¯æŒä¸åŒæ ¼å¼æ–‡ä»¶çš„æ··åˆåˆ†æ
    
    Args:
        files_data: æ–‡ä»¶æ•°æ®åˆ—è¡¨
        analysis_query: åˆ†ææŸ¥è¯¢
        model_config: æ¨¡å‹é…ç½®
        api_key: APIå¯†é’¥
    
    Returns:
        Dict: åˆ†æç»“æœ
    """
    try:
        # åˆå¹¶æ‰€æœ‰æ–‡ä»¶æ•°æ®
        merged_df = merge_multiple_files(files_data, merge_type="concat")
        
        if merged_df.empty:
            return {"answer": "æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®æ–‡ä»¶å¯ä¾›åˆ†æ"}
        
        # ä½¿ç”¨ç°æœ‰çš„æ•°æ®åˆ†ææ™ºèƒ½ä½“è¿›è¡Œåˆ†æ
        return dataframe_agent(merged_df, analysis_query, model_config, api_key)
        
    except Exception as e:
        return {"answer": f"æ··åˆæ•°æ®åˆ†æå¤±è´¥: {str(e)}"}


def get_file_info(file_obj, file_type: str) -> Dict:
    """è·å–æ–‡ä»¶ä¿¡æ¯
    
    Args:
        file_obj: æ–‡ä»¶å¯¹è±¡
        file_type: æ–‡ä»¶ç±»å‹
    
    Returns:
        Dict: æ–‡ä»¶ä¿¡æ¯
    """
    info = {
        'name': getattr(file_obj, 'name', 'æœªçŸ¥æ–‡ä»¶'),
        'type': file_type,
        'sheets': []
    }
    
    try:
        if file_type == 'excel':
            # é‡ç½®æ–‡ä»¶æŒ‡é’ˆåˆ°å¼€å§‹ä½ç½®
            if hasattr(file_obj, 'seek'):
                file_obj.seek(0)
            wb = openpyxl.load_workbook(file_obj)
            info['sheets'] = wb.sheetnames
        elif file_type == 'csv':
            # CSVæ–‡ä»¶åªæœ‰ä¸€ä¸ª"å·¥ä½œè¡¨"
            info['sheets'] = ['é»˜è®¤']
    except Exception as e:
        print(f"è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {e}")
        # å¦‚æœæ˜¯Excelæ–‡ä»¶è¯»å–å¤±è´¥ï¼Œæä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        if file_type == 'excel':
            info['error'] = f"Excelæ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}"
        
    return info


# ==================== å†å²è®°å½•ç®¡ç†åŠŸèƒ½ ====================

def init_history_database():
    """åˆå§‹åŒ–å†å²è®°å½•æ•°æ®åº“"""
    db_path = Path("analysis_history.db")
    
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # åˆ›å»ºå†å²è®°å½•è¡¨
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS analysis_history (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            timestamp TEXT NOT NULL,
            query TEXT NOT NULL,
            model_used TEXT NOT NULL,
            data_info TEXT,
            result_text TEXT,
            result_data TEXT,
            charts_info TEXT,
            session_id TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    conn.commit()
    conn.close()


def save_analysis_history(query: str, model_used: str, data_info: Dict, 
                         result: Dict, session_id: str = None) -> bool:
    """ä¿å­˜åˆ†æå†å²è®°å½•
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        model_used: ä½¿ç”¨çš„æ¨¡å‹
        data_info: æ•°æ®ä¿¡æ¯
        result: åˆ†æç»“æœ
        session_id: ä¼šè¯ID
    
    Returns:
        bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
    """
    try:
        init_history_database()
        
        conn = sqlite3.connect("analysis_history.db")
        cursor = conn.cursor()
        
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # æå–å›¾è¡¨ä¿¡æ¯
        charts_info = {}
        if "bar" in result:
            charts_info["bar"] = True
        if "line" in result:
            charts_info["line"] = True
        if "table" in result:
            charts_info["table"] = True
            
        cursor.execute('''
            INSERT INTO analysis_history 
            (timestamp, query, model_used, data_info, result_text, result_data, charts_info, session_id)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            timestamp,
            query,
            model_used,
            json.dumps(data_info, ensure_ascii=False),
            result.get("answer", ""),
            json.dumps(result, ensure_ascii=False),
            json.dumps(charts_info, ensure_ascii=False),
            session_id or "default"
        ))
        
        conn.commit()
        conn.close()
        return True
        
    except Exception as e:
        print(f"ä¿å­˜å†å²è®°å½•å¤±è´¥: {e}")
        return False


def get_analysis_history(limit: int = 50, session_id: str = None) -> List[Dict]:
    """è·å–åˆ†æå†å²è®°å½•
    
    Args:
        limit: è¿”å›è®°å½•æ•°é‡é™åˆ¶
        session_id: ä¼šè¯IDè¿‡æ»¤
    
    Returns:
        List[Dict]: å†å²è®°å½•åˆ—è¡¨
    """
    try:
        if not Path("analysis_history.db").exists():
            return []
            
        conn = sqlite3.connect("analysis_history.db")
        cursor = conn.cursor()
        
        if session_id:
            cursor.execute('''
                SELECT * FROM analysis_history 
                WHERE session_id = ?
                ORDER BY created_at DESC 
                LIMIT ?
            ''', (session_id, limit))
        else:
            cursor.execute('''
                SELECT * FROM analysis_history 
                ORDER BY created_at DESC 
                LIMIT ?
            ''', (limit,))
        
        columns = [description[0] for description in cursor.description]
        rows = cursor.fetchall()
        
        history = []
        for row in rows:
            record = dict(zip(columns, row))
            
            # è§£æJSONå­—æ®µ
            try:
                record['data_info'] = json.loads(record['data_info']) if record['data_info'] else {}
                record['result_data'] = json.loads(record['result_data']) if record['result_data'] else {}
                record['charts_info'] = json.loads(record['charts_info']) if record['charts_info'] else {}
            except:
                pass
                
            history.append(record)
        
        conn.close()
        return history
        
    except Exception as e:
        print(f"è·å–å†å²è®°å½•å¤±è´¥: {e}")
        return []


def delete_analysis_history(record_id: int = None, session_id: str = None, 
                           days_old: int = None) -> bool:
    """åˆ é™¤åˆ†æå†å²è®°å½•
    
    Args:
        record_id: ç‰¹å®šè®°å½•ID
        session_id: ä¼šè¯IDï¼ˆåˆ é™¤è¯¥ä¼šè¯æ‰€æœ‰è®°å½•ï¼‰
        days_old: åˆ é™¤å¤šå°‘å¤©å‰çš„è®°å½•
    
    Returns:
        bool: åˆ é™¤æ˜¯å¦æˆåŠŸ
    """
    try:
        if not Path("analysis_history.db").exists():
            return True
            
        conn = sqlite3.connect("analysis_history.db")
        cursor = conn.cursor()
        
        if record_id:
            cursor.execute('DELETE FROM analysis_history WHERE id = ?', (record_id,))
        elif session_id:
            cursor.execute('DELETE FROM analysis_history WHERE session_id = ?', (session_id,))
        elif days_old:
            cutoff_date = datetime.now() - pd.Timedelta(days=days_old)
            cursor.execute('DELETE FROM analysis_history WHERE created_at < ?', 
                         (cutoff_date.strftime("%Y-%m-%d %H:%M:%S"),))
        else:
            # åˆ é™¤æ‰€æœ‰è®°å½•
            cursor.execute('DELETE FROM analysis_history')
        
        conn.commit()
        conn.close()
        return True
        
    except Exception as e:
        print(f"åˆ é™¤å†å²è®°å½•å¤±è´¥: {e}")
        return False


def get_history_statistics() -> Dict:
    """è·å–å†å²è®°å½•ç»Ÿè®¡ä¿¡æ¯
    
    Returns:
        Dict: ç»Ÿè®¡ä¿¡æ¯
    """
    try:
        if not Path("analysis_history.db").exists():
            return {"total_records": 0, "total_sessions": 0, "most_used_model": "æ— ", "recent_records": 0}
            
        conn = sqlite3.connect("analysis_history.db")
        cursor = conn.cursor()
        
        # è·å–åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        cursor.execute('SELECT COUNT(*) FROM analysis_history')
        total_records = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(DISTINCT session_id) FROM analysis_history')
        total_sessions = cursor.fetchone()[0]
        
        cursor.execute('''
            SELECT model_used, COUNT(*) as count 
            FROM analysis_history 
            GROUP BY model_used 
            ORDER BY count DESC 
            LIMIT 1
        ''')
        most_used = cursor.fetchone()
        most_used_model = most_used[0] if most_used else "æ— "
        
        # è·å–æœ€è¿‘7å¤©çš„è®°å½•æ•°
        cursor.execute('''
            SELECT COUNT(*) FROM analysis_history 
            WHERE created_at >= datetime('now', '-7 days')
        ''')
        recent_records = cursor.fetchone()[0]
        
        conn.close()
        return {
            "total_records": total_records,
            "total_sessions": total_sessions,
            "most_used_model": most_used_model,
            "recent_records": recent_records
        }
        
    except Exception as e:
        print(f"è·å–å†å²ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        return {
            "total_records": 0,
            "total_sessions": 0,
            "most_used_model": "æ— ",
            "recent_records": 0
        }


def analyze_mixed_format_data(files) -> Dict:
    """åˆ†ææ··åˆæ ¼å¼æ–‡ä»¶æ•°æ®
    
    Args:
        files: ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨
    
    Returns:
        Dict: åˆ†æç»“æœå­—å…¸
    """
    analysis_results = {}
    
    for uploaded_file in files:
        try:
            file_name = uploaded_file.name
            file_extension = file_name.split('.')[-1].lower()
            
            # åˆ¤æ–­æ–‡ä»¶ç±»å‹å¹¶è¯»å–æ•°æ®
            if file_extension in ["xlsx", "xls", "xlsm", "xlsb", "xltx", "xltm"]:
                # é‡ç½®æ–‡ä»¶æŒ‡é’ˆåˆ°å¼€å§‹ä½ç½®
                if hasattr(uploaded_file, 'seek'):
                    uploaded_file.seek(0)
                
                # å°è¯•å¤šç§å¼•æ“è¯»å–Excelæ–‡ä»¶
                df = None
                engines = ['openpyxl', 'xlrd', None]
                
                for engine in engines:
                    try:
                        if hasattr(uploaded_file, 'seek'):
                            uploaded_file.seek(0)
                        if engine:
                            df = pd.read_excel(uploaded_file, engine=engine)
                        else:
                            df = pd.read_excel(uploaded_file)
                        break
                    except Exception:
                        continue
                
                if df is None:
                    raise Exception(f"æ— æ³•è¯»å–Excelæ–‡ä»¶ {file_name}ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼")
                file_type = "Excel"
            elif file_extension == "csv":
                df = pd.read_csv(uploaded_file)
                file_type = "CSV"
            elif file_extension == "txt":
                df = pd.read_csv(uploaded_file, sep='\t')
                file_type = "TXT"
            elif file_extension == "json":
                df = pd.read_json(uploaded_file)
                file_type = "JSON"
            else:
                continue
            
            # åˆ†ææ•°æ®ç‰¹å¾
            numeric_columns = len(df.select_dtypes(include=['number']).columns)
            text_columns = len(df.select_dtypes(include=['object']).columns)
            date_columns = len(df.select_dtypes(include=['datetime']).columns)
            
            analysis_results[file_name] = {
                'file_type': file_type,
                'rows': len(df),
                'columns': len(df.columns),
                'numeric_columns': numeric_columns,
                'text_columns': text_columns,
                'date_columns': date_columns,
                'data_preview': df.head(),
                'dataframe': df
            }
            
        except Exception as e:
            print(f"åˆ†ææ–‡ä»¶ {uploaded_file.name} å¤±è´¥: {e}")
            continue
    
    return analysis_results
